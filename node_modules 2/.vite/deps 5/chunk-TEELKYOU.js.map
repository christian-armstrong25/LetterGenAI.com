{
  "version": 3,
  "sources": ["../../langchain/dist/chat_models/openai.js", "../../langchain/dist/chat_models/base.js", "../../langchain/dist/util/prompt-layer.js"],
  "sourcesContent": ["import { Configuration, OpenAIApi, } from \"openai\";\nimport { getEnvironmentVariable, isNode } from \"../util/env.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseChatModel } from \"./base.js\";\nimport { AIChatMessage, ChatMessage, HumanChatMessage, SystemChatMessage, } from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nimport { promptLayerTrackRequest } from \"../util/prompt-layer.js\";\nfunction messageTypeToOpenAIRole(type) {\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\nfunction openAIResponseToChatMessage(role, text) {\n    switch (role) {\n        case \"user\":\n            return new HumanChatMessage(text);\n        case \"assistant\":\n            return new AIChatMessage(text);\n        case \"system\":\n            return new SystemChatMessage(text);\n        default:\n            return new ChatMessage(text, role ?? \"unknown\");\n    }\n}\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class ChatOpenAI extends BaseChatModel {\n    get callKeys() {\n        return [\"stop\", \"signal\", \"timeout\", \"options\"];\n    }\n    constructor(fields, configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const apiKey = fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n        const azureApiKey = fields?.azureOpenAIApiKey ??\n            getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n        if (!azureApiKey && !apiKey) {\n            throw new Error(\"(Azure) OpenAI API key not found\");\n        }\n        const azureApiInstanceName = fields?.azureOpenAIApiInstanceName ??\n            getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        const azureApiDeploymentName = fields?.azureOpenAIApiDeploymentName ??\n            getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n        const azureApiVersion = fields?.azureOpenAIApiVersion ??\n            getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        this.azureOpenAIApiVersion = azureApiVersion;\n        this.azureOpenAIApiKey = azureApiKey;\n        this.azureOpenAIApiInstanceName = azureApiInstanceName;\n        this.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n        if (this.streaming && this.n > 1) {\n            throw new Error(\"Cannot stream results when n > 1\");\n        }\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n        }\n        this.clientConfig = {\n            apiKey,\n            ...configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams() {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /** @ignore */\n    async _generate(messages, options, runManager) {\n        const tokenUsage = {};\n        if (this.stop && options?.stop) {\n            throw new Error(\"Stop found in input and default params\");\n        }\n        const params = this.invocationParams();\n        params.stop = options?.stop ?? params.stop;\n        const messagesMapped = messages.map((message) => ({\n            role: messageTypeToOpenAIRole(message._getType()),\n            content: message.text,\n            name: message.name,\n        }));\n        const data = params.stream\n            ? await new Promise((resolve, reject) => {\n                let response;\n                let rejected = false;\n                let resolved = false;\n                this.completionWithRetry({\n                    ...params,\n                    messages: messagesMapped,\n                }, {\n                    signal: options?.signal,\n                    ...options?.options,\n                    adapter: fetchAdapter,\n                    responseType: \"stream\",\n                    onmessage: (event) => {\n                        if (event.data?.trim?.() === \"[DONE]\") {\n                            if (resolved) {\n                                return;\n                            }\n                            resolved = true;\n                            resolve(response);\n                        }\n                        else {\n                            const message = JSON.parse(event.data);\n                            // on the first message set the response properties\n                            if (!response) {\n                                response = {\n                                    id: message.id,\n                                    object: message.object,\n                                    created: message.created,\n                                    model: message.model,\n                                    choices: [],\n                                };\n                            }\n                            // on all messages, update choice\n                            for (const part of message.choices) {\n                                if (part != null) {\n                                    let choice = response.choices.find((c) => c.index === part.index);\n                                    if (!choice) {\n                                        choice = {\n                                            index: part.index,\n                                            finish_reason: part.finish_reason ?? undefined,\n                                        };\n                                        response.choices[part.index] = choice;\n                                    }\n                                    if (!choice.message) {\n                                        choice.message = {\n                                            role: part.delta\n                                                ?.role,\n                                            content: part.delta?.content ?? \"\",\n                                        };\n                                    }\n                                    choice.message.content += part.delta?.content ?? \"\";\n                                    // TODO this should pass part.index to the callback\n                                    // when that's supported there\n                                    // eslint-disable-next-line no-void\n                                    void runManager?.handleLLMNewToken(part.delta?.content ?? \"\");\n                                }\n                            }\n                            // when all messages are finished, resolve\n                            if (!resolved &&\n                                message.choices.every((c) => c.finish_reason != null)) {\n                                resolved = true;\n                                resolve(response);\n                            }\n                        }\n                    },\n                }).catch((error) => {\n                    if (!rejected) {\n                        rejected = true;\n                        reject(error);\n                    }\n                });\n            })\n            : await this.completionWithRetry({\n                ...params,\n                messages: messagesMapped,\n            }, {\n                signal: options?.signal,\n                ...options?.options,\n            });\n        const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n        if (completionTokens) {\n            tokenUsage.completionTokens =\n                (tokenUsage.completionTokens ?? 0) + completionTokens;\n        }\n        if (promptTokens) {\n            tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n        }\n        if (totalTokens) {\n            tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n        }\n        const generations = [];\n        for (const part of data.choices) {\n            const role = part.message?.role ?? undefined;\n            const text = part.message?.content ?? \"\";\n            generations.push({\n                text,\n                message: openAIResponseToChatMessage(role, text),\n            });\n        }\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.text);\n            const roleCount = await this.getNumTokens(messageTypeToOpenAIRole(message._getType()));\n            const nameCount = message.name !== undefined\n                ? tokensPerName + (await this.getNumTokens(message.name))\n                : 0;\n            const count = textCount + tokensPerMessage + roleCount + nameCount;\n            totalCount += count;\n            return count;\n        }));\n        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n        return { totalCount, countPerMessage };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const endpoint = this.azureOpenAIApiKey\n                ? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n                : this.clientConfig.basePath;\n            const clientConfig = new Configuration({\n                ...this.clientConfig,\n                basePath: endpoint,\n                baseOptions: {\n                    timeout: this.timeout,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new OpenAIApi(clientConfig);\n        }\n        const axiosOptions = {\n            adapter: isNode() ? undefined : fetchAdapter,\n            ...this.clientConfig.baseOptions,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            axiosOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...axiosOptions.headers,\n            };\n            axiosOptions.params = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...axiosOptions.params,\n            };\n        }\n        return this.caller\n            .call(this.client.createChatCompletion.bind(this.client), request, axiosOptions)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n}\nexport class PromptLayerChatOpenAI extends ChatOpenAI {\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnPromptLayerId\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                (typeof process !== \"undefined\"\n                    ? // eslint-disable-next-line no-process-env\n                        process.env?.PROMPTLAYER_API_KEY\n                    : undefined);\n        this.plTags = fields?.plTags ?? [];\n        this.returnPromptLayerId = fields?.returnPromptLayerId ?? false;\n    }\n    async _generate(messages, options, runManager) {\n        const requestStartTime = Date.now();\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else if (options?.timeout && !options.signal) {\n            parsedOptions = {\n                ...options,\n                signal: AbortSignal.timeout(options.timeout),\n            };\n        }\n        else {\n            parsedOptions = options ?? {};\n        }\n        const generatedResponses = await super._generate(messages, parsedOptions, runManager);\n        const requestEndTime = Date.now();\n        const _convertMessageToDict = (message) => {\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            let messageDict;\n            if (message._getType() === \"human\") {\n                messageDict = { role: \"user\", content: message.text };\n            }\n            else if (message._getType() === \"ai\") {\n                messageDict = { role: \"assistant\", content: message.text };\n            }\n            else if (message._getType() === \"system\") {\n                messageDict = { role: \"system\", content: message.text };\n            }\n            else if (message._getType() === \"generic\") {\n                messageDict = {\n                    role: message.role,\n                    content: message.text,\n                };\n            }\n            else {\n                throw new Error(`Got unknown type ${message}`);\n            }\n            return messageDict;\n        };\n        const _createMessageDicts = (messages, callOptions) => {\n            const params = {\n                ...this.invocationParams(),\n                model: this.modelName,\n            };\n            if (callOptions?.stop) {\n                if (Object.keys(params).includes(\"stop\")) {\n                    throw new Error(\"`stop` found in both the input and default params.\");\n                }\n            }\n            const messageDicts = messages.map((message) => _convertMessageToDict(message));\n            return messageDicts;\n        };\n        for (let i = 0; i < generatedResponses.generations.length; i += 1) {\n            const generation = generatedResponses.generations[i];\n            const messageDicts = _createMessageDicts(messages, parsedOptions);\n            let promptLayerRequestId;\n            const parsedResp = [\n                {\n                    content: generation.text,\n                    role: messageTypeToOpenAIRole(generation.message._getType()),\n                },\n            ];\n            const promptLayerRespBody = await promptLayerTrackRequest(this.caller, \"langchain.PromptLayerChatOpenAI\", messageDicts, this._identifyingParams(), this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n            if (this.returnPromptLayerId === true) {\n                if (promptLayerRespBody.success === true) {\n                    promptLayerRequestId = promptLayerRespBody.request_id;\n                }\n                if (!generation.generationInfo ||\n                    typeof generation.generationInfo !== \"object\") {\n                    generation.generationInfo = {};\n                }\n                generation.generationInfo.promptLayerRequestId = promptLayerRequestId;\n            }\n        }\n        return generatedResponses;\n    }\n}\n", "import { AIChatMessage, HumanChatMessage, RUN_KEY, } from \"../schema/index.js\";\nimport { BaseLanguageModel, } from \"../base_language/index.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n    }\n    async generate(messages, options, callbacks) {\n        const generations = [];\n        const llmOutputs = [];\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else if (options?.timeout && !options.signal) {\n            parsedOptions = {\n                ...options,\n                signal: AbortSignal.timeout(options.timeout),\n            };\n        }\n        else {\n            parsedOptions = options ?? {};\n        }\n        const callbackManager_ = await CallbackManager.configure(callbacks, this.callbacks, { verbose: this.verbose });\n        const invocationParams = { invocation_params: this?.invocationParams() };\n        const runManager = await callbackManager_?.handleChatModelStart({ name: this._llmType() }, messages, undefined, undefined, invocationParams);\n        try {\n            const results = await Promise.all(messages.map((messageList) => this._generate(messageList, parsedOptions, runManager)));\n            for (const result of results) {\n                if (result.llmOutput) {\n                    llmOutputs.push(result.llmOutput);\n                }\n                generations.push(result.generations);\n            }\n        }\n        catch (err) {\n            await runManager?.handleLLMError(err);\n            throw err;\n        }\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        await runManager?.handleLLMEnd(output);\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManager ? { runId: runManager?.runId } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams() {\n        return {};\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n    async predictMessages(messages, options, callbacks) {\n        return this.call(messages, options, callbacks);\n    }\n    async predict(text, options, callbacks) {\n        const message = new HumanChatMessage(text);\n        const result = await this.call([message], options, callbacks);\n        return result.text;\n    }\n}\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIChatMessage(text);\n        return {\n            generations: [\n                {\n                    text: message.text,\n                    message,\n                },\n            ],\n        };\n    }\n}\n", "export const promptLayerTrackRequest = async (callerFunc, functionName, prompt, kwargs, plTags, \n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nrequestResponse, startTime, endTime, apiKey) => {\n    // https://github.com/MagnivOrg/promptlayer-js-helper\n    const promptLayerResp = await callerFunc.call(fetch, \"https://api.promptlayer.com/track-request\", {\n        method: \"POST\",\n        headers: {\n            \"Content-Type\": \"application/json\",\n            Accept: \"application/json\",\n        },\n        body: JSON.stringify({\n            function_name: functionName,\n            provider: \"langchain\",\n            args: prompt,\n            kwargs,\n            tags: plTags,\n            request_response: requestResponse,\n            request_start_time: Math.floor(startTime / 1000),\n            request_end_time: Math.floor(endTime / 1000),\n            api_key: apiKey,\n        }),\n    });\n    return promptLayerResp.json();\n};\n"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;;;;AAAA,oBAA0C;;;ACGnC,IAAM,gBAAN,cAA4B,kBAAkB;AAAA,EACjD,YAAY,QAAQ;AAChB,UAAM,MAAM;AAAA,EAChB;AAAA,EACA,MAAM,SAAS,UAAU,SAAS,WAAW;AAPjD;AAQQ,UAAM,cAAc,CAAC;AACrB,UAAM,aAAa,CAAC;AACpB,QAAI;AACJ,QAAI,MAAM,QAAQ,OAAO,GAAG;AACxB,sBAAgB,EAAE,MAAM,QAAQ;AAAA,IACpC,YACS,mCAAS,YAAW,CAAC,QAAQ,QAAQ;AAC1C,sBAAgB;AAAA,QACZ,GAAG;AAAA,QACH,QAAQ,YAAY,QAAQ,QAAQ,OAAO;AAAA,MAC/C;AAAA,IACJ,OACK;AACD,sBAAgB,WAAW,CAAC;AAAA,IAChC;AACA,UAAM,mBAAmB,MAAM,gBAAgB,UAAU,WAAW,KAAK,WAAW,EAAE,SAAS,KAAK,QAAQ,CAAC;AAC7G,UAAM,mBAAmB,EAAE,mBAAmB,6BAAM,mBAAmB;AACvE,UAAM,aAAa,OAAM,qDAAkB,qBAAqB,EAAE,MAAM,KAAK,SAAS,EAAE,GAAG,UAAU,QAAW,QAAW;AAC3H,QAAI;AACA,YAAM,UAAU,MAAM,QAAQ,IAAI,SAAS,IAAI,CAAC,gBAAgB,KAAK,UAAU,aAAa,eAAe,UAAU,CAAC,CAAC;AACvH,iBAAW,UAAU,SAAS;AAC1B,YAAI,OAAO,WAAW;AAClB,qBAAW,KAAK,OAAO,SAAS;AAAA,QACpC;AACA,oBAAY,KAAK,OAAO,WAAW;AAAA,MACvC;AAAA,IACJ,SACO,KAAP;AACI,aAAM,yCAAY,eAAe;AACjC,YAAM;AAAA,IACV;AACA,UAAM,SAAS;AAAA,MACX;AAAA,MACA,WAAW,WAAW,UAChB,UAAK,sBAAL,8BAAyB,GAAG,cAC5B;AAAA,IACV;AACA,WAAM,yCAAY,aAAa;AAC/B,WAAO,eAAe,QAAQ,SAAS;AAAA,MACnC,OAAO,aAAa,EAAE,OAAO,yCAAY,MAAM,IAAI;AAAA,MACnD,cAAc;AAAA,IAClB,CAAC;AACD,WAAO;AAAA,EACX;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,mBAAmB;AACf,WAAO,CAAC;AAAA,EACZ;AAAA,EACA,aAAa;AACT,WAAO;AAAA,EACX;AAAA,EACA,MAAM,eAAe,cAAc,SAAS,WAAW;AACnD,UAAM,iBAAiB,aAAa,IAAI,CAAC,gBAAgB,YAAY,eAAe,CAAC;AACrF,WAAO,KAAK,SAAS,gBAAgB,SAAS,SAAS;AAAA,EAC3D;AAAA,EACA,MAAM,KAAK,UAAU,SAAS,WAAW;AACrC,UAAM,SAAS,MAAM,KAAK,SAAS,CAAC,QAAQ,GAAG,SAAS,SAAS;AACjE,UAAM,cAAc,OAAO;AAC3B,WAAO,YAAY,CAAC,EAAE,CAAC,EAAE;AAAA,EAC7B;AAAA,EACA,MAAM,WAAW,aAAa,SAAS,WAAW;AAC9C,UAAM,iBAAiB,YAAY,eAAe;AAClD,WAAO,KAAK,KAAK,gBAAgB,SAAS,SAAS;AAAA,EACvD;AAAA,EACA,MAAM,gBAAgB,UAAU,SAAS,WAAW;AAChD,WAAO,KAAK,KAAK,UAAU,SAAS,SAAS;AAAA,EACjD;AAAA,EACA,MAAM,QAAQ,MAAM,SAAS,WAAW;AACpC,UAAM,UAAU,IAAI,iBAAiB,IAAI;AACzC,UAAM,SAAS,MAAM,KAAK,KAAK,CAAC,OAAO,GAAG,SAAS,SAAS;AAC5D,WAAO,OAAO;AAAA,EAClB;AACJ;;;ACnFO,IAAM,0BAA0B,OAAO,YAAY,cAAc,QAAQ,QAAQ,QAExF,iBAAiB,WAAW,SAAS,WAAW;AAE5C,QAAM,kBAAkB,MAAM,WAAW,KAAK,OAAO,6CAA6C;AAAA,IAC9F,QAAQ;AAAA,IACR,SAAS;AAAA,MACL,gBAAgB;AAAA,MAChB,QAAQ;AAAA,IACZ;AAAA,IACA,MAAM,KAAK,UAAU;AAAA,MACjB,eAAe;AAAA,MACf,UAAU;AAAA,MACV,MAAM;AAAA,MACN;AAAA,MACA,MAAM;AAAA,MACN,kBAAkB;AAAA,MAClB,oBAAoB,KAAK,MAAM,YAAY,GAAI;AAAA,MAC/C,kBAAkB,KAAK,MAAM,UAAU,GAAI;AAAA,MAC3C,SAAS;AAAA,IACb,CAAC;AAAA,EACL,CAAC;AACD,SAAO,gBAAgB,KAAK;AAChC;;;AFhBA,SAAS,wBAAwB,MAAM;AACnC,UAAQ,MAAM;AAAA,IACV,KAAK;AACD,aAAO;AAAA,IACX,KAAK;AACD,aAAO;AAAA,IACX,KAAK;AACD,aAAO;AAAA,IACX;AACI,YAAM,IAAI,MAAM,yBAAyB,MAAM;AAAA,EACvD;AACJ;AACA,SAAS,4BAA4B,MAAM,MAAM;AAC7C,UAAQ,MAAM;AAAA,IACV,KAAK;AACD,aAAO,IAAI,iBAAiB,IAAI;AAAA,IACpC,KAAK;AACD,aAAO,IAAI,cAAc,IAAI;AAAA,IACjC,KAAK;AACD,aAAO,IAAI,kBAAkB,IAAI;AAAA,IACrC;AACI,aAAO,IAAI,YAAY,MAAM,QAAQ,SAAS;AAAA,EACtD;AACJ;AAmBO,IAAM,aAAN,cAAyB,cAAc;AAAA,EAC1C,IAAI,WAAW;AACX,WAAO,CAAC,QAAQ,UAAU,WAAW,SAAS;AAAA,EAClD;AAAA,EACA,YAAY,QAAQ,eAAe;AAC/B,UAAM,UAAU,CAAC,CAAC;AAClB,WAAO,eAAe,MAAM,eAAe;AAAA,MACvC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,QAAQ;AAAA,MAChC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,oBAAoB;AAAA,MAC5C,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,mBAAmB;AAAA,MAC3C,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,KAAK;AAAA,MAC7B,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,aAAa;AAAA,MACrC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,aAAa;AAAA,MACrC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,eAAe;AAAA,MACvC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,QAAQ;AAAA,MAChC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,WAAW;AAAA,MACnC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,aAAa;AAAA,MACrC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,aAAa;AAAA,MACrC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,yBAAyB;AAAA,MACjD,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,qBAAqB;AAAA,MAC7C,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,8BAA8B;AAAA,MACtD,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,gCAAgC;AAAA,MACxD,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,UAAU;AAAA,MAClC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,gBAAgB;AAAA,MACxC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,UAAM,UAAS,iCAAQ,iBAAgB,uBAAuB,gBAAgB;AAC9E,UAAM,eAAc,iCAAQ,sBACxB,uBAAuB,sBAAsB;AACjD,QAAI,CAAC,eAAe,CAAC,QAAQ;AACzB,YAAM,IAAI,MAAM,kCAAkC;AAAA,IACtD;AACA,UAAM,wBAAuB,iCAAQ,+BACjC,uBAAuB,gCAAgC;AAC3D,UAAM,0BAAyB,iCAAQ,iCACnC,uBAAuB,kCAAkC;AAC7D,UAAM,mBAAkB,iCAAQ,0BAC5B,uBAAuB,0BAA0B;AACrD,SAAK,aAAY,iCAAQ,cAAa,KAAK;AAC3C,SAAK,eAAc,iCAAQ,gBAAe,CAAC;AAC3C,SAAK,UAAU,iCAAQ;AACvB,SAAK,eAAc,iCAAQ,gBAAe,KAAK;AAC/C,SAAK,QAAO,iCAAQ,SAAQ,KAAK;AACjC,SAAK,oBAAmB,iCAAQ,qBAAoB,KAAK;AACzD,SAAK,mBAAkB,iCAAQ,oBAAmB,KAAK;AACvD,SAAK,YAAY,iCAAQ;AACzB,SAAK,KAAI,iCAAQ,MAAK,KAAK;AAC3B,SAAK,YAAY,iCAAQ;AACzB,SAAK,OAAO,iCAAQ;AACpB,SAAK,aAAY,iCAAQ,cAAa;AACtC,SAAK,wBAAwB;AAC7B,SAAK,oBAAoB;AACzB,SAAK,6BAA6B;AAClC,SAAK,+BAA+B;AACpC,QAAI,KAAK,aAAa,KAAK,IAAI,GAAG;AAC9B,YAAM,IAAI,MAAM,kCAAkC;AAAA,IACtD;AACA,QAAI,KAAK,mBAAmB;AACxB,UAAI,CAAC,KAAK,4BAA4B;AAClC,cAAM,IAAI,MAAM,0CAA0C;AAAA,MAC9D;AACA,UAAI,CAAC,KAAK,8BAA8B;AACpC,cAAM,IAAI,MAAM,4CAA4C;AAAA,MAChE;AACA,UAAI,CAAC,KAAK,uBAAuB;AAC7B,cAAM,IAAI,MAAM,oCAAoC;AAAA,MACxD;AAAA,IACJ;AACA,SAAK,eAAe;AAAA,MAChB;AAAA,MACA,GAAG;AAAA,IACP;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA,EAIA,mBAAmB;AACf,WAAO;AAAA,MACH,OAAO,KAAK;AAAA,MACZ,aAAa,KAAK;AAAA,MAClB,OAAO,KAAK;AAAA,MACZ,mBAAmB,KAAK;AAAA,MACxB,kBAAkB,KAAK;AAAA,MACvB,YAAY,KAAK,cAAc,KAAK,SAAY,KAAK;AAAA,MACrD,GAAG,KAAK;AAAA,MACR,YAAY,KAAK;AAAA,MACjB,MAAM,KAAK;AAAA,MACX,QAAQ,KAAK;AAAA,MACb,GAAG,KAAK;AAAA,IACZ;AAAA,EACJ;AAAA;AAAA,EAEA,qBAAqB;AACjB,WAAO;AAAA,MACH,YAAY,KAAK;AAAA,MACjB,GAAG,KAAK,iBAAiB;AAAA,MACzB,GAAG,KAAK;AAAA,IACZ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA,EAIA,oBAAoB;AAChB,WAAO,KAAK,mBAAmB;AAAA,EACnC;AAAA;AAAA,EAEA,MAAM,UAAU,UAAU,SAAS,YAAY;AAnPnD;AAoPQ,UAAM,aAAa,CAAC;AACpB,QAAI,KAAK,SAAQ,mCAAS,OAAM;AAC5B,YAAM,IAAI,MAAM,wCAAwC;AAAA,IAC5D;AACA,UAAM,SAAS,KAAK,iBAAiB;AACrC,WAAO,QAAO,mCAAS,SAAQ,OAAO;AACtC,UAAM,iBAAiB,SAAS,IAAI,CAAC,aAAa;AAAA,MAC9C,MAAM,wBAAwB,QAAQ,SAAS,CAAC;AAAA,MAChD,SAAS,QAAQ;AAAA,MACjB,MAAM,QAAQ;AAAA,IAClB,EAAE;AACF,UAAM,OAAO,OAAO,SACd,MAAM,IAAI,QAAQ,CAAC,SAAS,WAAW;AACrC,UAAI;AACJ,UAAI,WAAW;AACf,UAAI,WAAW;AACf,WAAK,oBAAoB;AAAA,QACrB,GAAG;AAAA,QACH,UAAU;AAAA,MACd,GAAG;AAAA,QACC,QAAQ,mCAAS;AAAA,QACjB,GAAG,mCAAS;AAAA,QACZ,SAAS;AAAA,QACT,cAAc;AAAA,QACd,WAAW,CAAC,UAAU;AA5Q1C,cAAAA,KAAAC,KAAA;AA6QwB,gBAAIA,OAAAD,MAAA,MAAM,SAAN,gBAAAA,IAAY,SAAZ,gBAAAC,IAAA,KAAAD,UAAyB,UAAU;AACnC,gBAAI,UAAU;AACV;AAAA,YACJ;AACA,uBAAW;AACX,oBAAQ,QAAQ;AAAA,UACpB,OACK;AACD,kBAAM,UAAU,KAAK,MAAM,MAAM,IAAI;AAErC,gBAAI,CAAC,UAAU;AACX,yBAAW;AAAA,gBACP,IAAI,QAAQ;AAAA,gBACZ,QAAQ,QAAQ;AAAA,gBAChB,SAAS,QAAQ;AAAA,gBACjB,OAAO,QAAQ;AAAA,gBACf,SAAS,CAAC;AAAA,cACd;AAAA,YACJ;AAEA,uBAAW,QAAQ,QAAQ,SAAS;AAChC,kBAAI,QAAQ,MAAM;AACd,oBAAI,SAAS,SAAS,QAAQ,KAAK,CAAC,MAAM,EAAE,UAAU,KAAK,KAAK;AAChE,oBAAI,CAAC,QAAQ;AACT,2BAAS;AAAA,oBACL,OAAO,KAAK;AAAA,oBACZ,eAAe,KAAK,iBAAiB;AAAA,kBACzC;AACA,2BAAS,QAAQ,KAAK,KAAK,IAAI;AAAA,gBACnC;AACA,oBAAI,CAAC,OAAO,SAAS;AACjB,yBAAO,UAAU;AAAA,oBACb,OAAM,UAAK,UAAL,mBACA;AAAA,oBACN,WAAS,UAAK,UAAL,mBAAY,YAAW;AAAA,kBACpC;AAAA,gBACJ;AACA,uBAAO,QAAQ,aAAW,UAAK,UAAL,mBAAY,YAAW;AAIjD,sBAAK,yCAAY,oBAAkB,UAAK,UAAL,mBAAY,YAAW;AAAA,cAC9D;AAAA,YACJ;AAEA,gBAAI,CAAC,YACD,QAAQ,QAAQ,MAAM,CAAC,MAAM,EAAE,iBAAiB,IAAI,GAAG;AACvD,yBAAW;AACX,sBAAQ,QAAQ;AAAA,YACpB;AAAA,UACJ;AAAA,QACJ;AAAA,MACJ,CAAC,EAAE,MAAM,CAAC,UAAU;AAChB,YAAI,CAAC,UAAU;AACX,qBAAW;AACX,iBAAO,KAAK;AAAA,QAChB;AAAA,MACJ,CAAC;AAAA,IACL,CAAC,IACC,MAAM,KAAK,oBAAoB;AAAA,MAC7B,GAAG;AAAA,MACH,UAAU;AAAA,IACd,GAAG;AAAA,MACC,QAAQ,mCAAS;AAAA,MACjB,GAAG,mCAAS;AAAA,IAChB,CAAC;AACL,UAAM,EAAE,mBAAmB,kBAAkB,eAAe,cAAc,cAAc,YAAa,IAAI,KAAK,SAAS,CAAC;AACxH,QAAI,kBAAkB;AAClB,iBAAW,oBACN,WAAW,oBAAoB,KAAK;AAAA,IAC7C;AACA,QAAI,cAAc;AACd,iBAAW,gBAAgB,WAAW,gBAAgB,KAAK;AAAA,IAC/D;AACA,QAAI,aAAa;AACb,iBAAW,eAAe,WAAW,eAAe,KAAK;AAAA,IAC7D;AACA,UAAM,cAAc,CAAC;AACrB,eAAW,QAAQ,KAAK,SAAS;AAC7B,YAAM,SAAO,UAAK,YAAL,mBAAc,SAAQ;AACnC,YAAM,SAAO,UAAK,YAAL,mBAAc,YAAW;AACtC,kBAAY,KAAK;AAAA,QACb;AAAA,QACA,SAAS,4BAA4B,MAAM,IAAI;AAAA,MACnD,CAAC;AAAA,IACL;AACA,WAAO;AAAA,MACH;AAAA,MACA,WAAW,EAAE,WAAW;AAAA,IAC5B;AAAA,EACJ;AAAA,EACA,MAAM,yBAAyB,UAAU;AACrC,QAAI,aAAa;AACjB,QAAI,mBAAmB;AACvB,QAAI,gBAAgB;AAEpB,QAAI,wBAAwB,KAAK,SAAS,MAAM,iBAAiB;AAC7D,yBAAmB;AACnB,sBAAgB;AAAA,IACpB,WACS,wBAAwB,KAAK,SAAS,EAAE,WAAW,OAAO,GAAG;AAClE,yBAAmB;AACnB,sBAAgB;AAAA,IACpB;AACA,UAAM,kBAAkB,MAAM,QAAQ,IAAI,SAAS,IAAI,OAAO,YAAY;AACtE,YAAM,YAAY,MAAM,KAAK,aAAa,QAAQ,IAAI;AACtD,YAAM,YAAY,MAAM,KAAK,aAAa,wBAAwB,QAAQ,SAAS,CAAC,CAAC;AACrF,YAAM,YAAY,QAAQ,SAAS,SAC7B,gBAAiB,MAAM,KAAK,aAAa,QAAQ,IAAI,IACrD;AACN,YAAM,QAAQ,YAAY,mBAAmB,YAAY;AACzD,oBAAc;AACd,aAAO;AAAA,IACX,CAAC,CAAC;AACF,kBAAc;AACd,WAAO,EAAE,YAAY,gBAAgB;AAAA,EACzC;AAAA;AAAA,EAEA,MAAM,oBAAoB,SAAS,SAAS;AACxC,QAAI,CAAC,KAAK,QAAQ;AACd,YAAM,WAAW,KAAK,oBAChB,WAAW,KAAK,kEAAkE,KAAK,iCACvF,KAAK,aAAa;AACxB,YAAM,eAAe,IAAI,4BAAc;AAAA,QACnC,GAAG,KAAK;AAAA,QACR,UAAU;AAAA,QACV,aAAa;AAAA,UACT,SAAS,KAAK;AAAA,UACd,GAAG,KAAK,aAAa;AAAA,QACzB;AAAA,MACJ,CAAC;AACD,WAAK,SAAS,IAAI,wBAAU,YAAY;AAAA,IAC5C;AACA,UAAM,eAAe;AAAA,MACjB,SAAS,OAAO,IAAI,SAAY;AAAA,MAChC,GAAG,KAAK,aAAa;AAAA,MACrB,GAAG;AAAA,IACP;AACA,QAAI,KAAK,mBAAmB;AACxB,mBAAa,UAAU;AAAA,QACnB,WAAW,KAAK;AAAA,QAChB,GAAG,aAAa;AAAA,MACpB;AACA,mBAAa,SAAS;AAAA,QAClB,eAAe,KAAK;AAAA,QACpB,GAAG,aAAa;AAAA,MACpB;AAAA,IACJ;AACA,WAAO,KAAK,OACP,KAAK,KAAK,OAAO,qBAAqB,KAAK,KAAK,MAAM,GAAG,SAAS,YAAY,EAC9E,KAAK,CAAC,QAAQ,IAAI,IAAI;AAAA,EAC/B;AAAA,EACA,WAAW;AACP,WAAO;AAAA,EACX;AAAA;AAAA,EAEA,qBAAqB,YAAY;AAC7B,WAAO,WAAW,OAAO,CAAC,KAAK,cAAc;AACzC,UAAI,aAAa,UAAU,YAAY;AACnC,YAAI,WAAW,oBACX,UAAU,WAAW,oBAAoB;AAC7C,YAAI,WAAW,gBAAgB,UAAU,WAAW,gBAAgB;AACpE,YAAI,WAAW,eAAe,UAAU,WAAW,eAAe;AAAA,MACtE;AACA,aAAO;AAAA,IACX,GAAG;AAAA,MACC,YAAY;AAAA,QACR,kBAAkB;AAAA,QAClB,cAAc;AAAA,QACd,aAAa;AAAA,MACjB;AAAA,IACJ,CAAC;AAAA,EACL;AACJ;AACO,IAAM,wBAAN,cAAoC,WAAW;AAAA,EAClD,YAAY,QAAQ;AA5bxB;AA6bQ,UAAM,MAAM;AACZ,WAAO,eAAe,MAAM,qBAAqB;AAAA,MAC7C,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,UAAU;AAAA,MAClC,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,WAAO,eAAe,MAAM,uBAAuB;AAAA,MAC/C,YAAY;AAAA,MACZ,cAAc;AAAA,MACd,UAAU;AAAA,MACV,OAAO;AAAA,IACX,CAAC;AACD,SAAK,qBACD,iCAAQ,uBACH,OAAO,YAAY;AAAA;AAAA,OAEZ,aAAQ,QAAR,mBAAa;AAAA,QACf;AACd,SAAK,UAAS,iCAAQ,WAAU,CAAC;AACjC,SAAK,uBAAsB,iCAAQ,wBAAuB;AAAA,EAC9D;AAAA,EACA,MAAM,UAAU,UAAU,SAAS,YAAY;AAC3C,UAAM,mBAAmB,KAAK,IAAI;AAClC,QAAI;AACJ,QAAI,MAAM,QAAQ,OAAO,GAAG;AACxB,sBAAgB,EAAE,MAAM,QAAQ;AAAA,IACpC,YACS,mCAAS,YAAW,CAAC,QAAQ,QAAQ;AAC1C,sBAAgB;AAAA,QACZ,GAAG;AAAA,QACH,QAAQ,YAAY,QAAQ,QAAQ,OAAO;AAAA,MAC/C;AAAA,IACJ,OACK;AACD,sBAAgB,WAAW,CAAC;AAAA,IAChC;AACA,UAAM,qBAAqB,MAAM,MAAM,UAAU,UAAU,eAAe,UAAU;AACpF,UAAM,iBAAiB,KAAK,IAAI;AAChC,UAAM,wBAAwB,CAAC,YAAY;AAEvC,UAAI;AACJ,UAAI,QAAQ,SAAS,MAAM,SAAS;AAChC,sBAAc,EAAE,MAAM,QAAQ,SAAS,QAAQ,KAAK;AAAA,MACxD,WACS,QAAQ,SAAS,MAAM,MAAM;AAClC,sBAAc,EAAE,MAAM,aAAa,SAAS,QAAQ,KAAK;AAAA,MAC7D,WACS,QAAQ,SAAS,MAAM,UAAU;AACtC,sBAAc,EAAE,MAAM,UAAU,SAAS,QAAQ,KAAK;AAAA,MAC1D,WACS,QAAQ,SAAS,MAAM,WAAW;AACvC,sBAAc;AAAA,UACV,MAAM,QAAQ;AAAA,UACd,SAAS,QAAQ;AAAA,QACrB;AAAA,MACJ,OACK;AACD,cAAM,IAAI,MAAM,oBAAoB,SAAS;AAAA,MACjD;AACA,aAAO;AAAA,IACX;AACA,UAAM,sBAAsB,CAACE,WAAU,gBAAgB;AACnD,YAAM,SAAS;AAAA,QACX,GAAG,KAAK,iBAAiB;AAAA,QACzB,OAAO,KAAK;AAAA,MAChB;AACA,UAAI,2CAAa,MAAM;AACnB,YAAI,OAAO,KAAK,MAAM,EAAE,SAAS,MAAM,GAAG;AACtC,gBAAM,IAAI,MAAM,oDAAoD;AAAA,QACxE;AAAA,MACJ;AACA,YAAM,eAAeA,UAAS,IAAI,CAAC,YAAY,sBAAsB,OAAO,CAAC;AAC7E,aAAO;AAAA,IACX;AACA,aAAS,IAAI,GAAG,IAAI,mBAAmB,YAAY,QAAQ,KAAK,GAAG;AAC/D,YAAM,aAAa,mBAAmB,YAAY,CAAC;AACnD,YAAM,eAAe,oBAAoB,UAAU,aAAa;AAChE,UAAI;AACJ,YAAM,aAAa;AAAA,QACf;AAAA,UACI,SAAS,WAAW;AAAA,UACpB,MAAM,wBAAwB,WAAW,QAAQ,SAAS,CAAC;AAAA,QAC/D;AAAA,MACJ;AACA,YAAM,sBAAsB,MAAM,wBAAwB,KAAK,QAAQ,mCAAmC,cAAc,KAAK,mBAAmB,GAAG,KAAK,QAAQ,YAAY,kBAAkB,gBAAgB,KAAK,iBAAiB;AACpO,UAAI,KAAK,wBAAwB,MAAM;AACnC,YAAI,oBAAoB,YAAY,MAAM;AACtC,iCAAuB,oBAAoB;AAAA,QAC/C;AACA,YAAI,CAAC,WAAW,kBACZ,OAAO,WAAW,mBAAmB,UAAU;AAC/C,qBAAW,iBAAiB,CAAC;AAAA,QACjC;AACA,mBAAW,eAAe,uBAAuB;AAAA,MACrD;AAAA,IACJ;AACA,WAAO;AAAA,EACX;AACJ;",
  "names": ["_a", "_b", "messages"]
}
